{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI Compliance Report Generator for Analytics Code & Business Decks\n",
        "\n",
        "This notebook demonstrates a fully local, multi-agent compliance review pipeline built with the **Google ADK** package. It scans either analytics code or business presentation text, maps potential issues to synthetic policy rules, and emits both machine-readable and human-readable compliance reports.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Setup: install dependencies and import ADK components\n",
        "!pip install -q google-adk\n",
        "\n",
        "import json\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "from typing import Any, Dict, List, Optional\n",
        "\n",
        "from google.adk.agents import Agent\n",
        "from google.adk.sessions import Session\n",
        "from google.adk.tools.function_tool import FunctionTool\n",
        "\n",
        "# Print the installed ADK version for traceability\n",
        "import importlib.metadata\n",
        "print('google-adk version:', importlib.metadata.version('google-adk'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regulation & Policy Knowledge Base\n",
        "\n",
        "We define a compact, in-notebook policy and regulation dictionary alongside policy bundles for the two supported business contexts.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "REGULATION_KB: Dict[str, Dict[str, str]] = {\n",
        "    'GDPR-PII-LOGGING': {\n",
        "        'name': 'GDPR \u2013 PII in Logs',\n",
        "        'category': 'privacy',\n",
        "        'description': 'Avoid logging directly identifiable personal data (emails, phone numbers, IDs).',\n",
        "        'severity_default': 'high',\n",
        "    },\n",
        "    'INT-PII-MASKING': {\n",
        "        'name': 'Internal \u2013 Mask PII',\n",
        "        'category': 'privacy',\n",
        "        'description': 'PII must be masked or hashed before storage or export.',\n",
        "        'severity_default': 'medium',\n",
        "    },\n",
        "    'MKT-CLAIMS-01': {\n",
        "        'name': 'Marketing \u2013 No Guaranteed Returns',\n",
        "        'category': 'marketing',\n",
        "        'description': 'Marketing material must not promise guaranteed returns or risk-free performance.',\n",
        "        'severity_default': 'high',\n",
        "    },\n",
        "    'MKT-DISCLAIMER-01': {\n",
        "        'name': 'Marketing \u2013 Required Disclaimer',\n",
        "        'category': 'marketing',\n",
        "        'description': 'Presentations containing performance metrics must include a standard disclaimer slide.',\n",
        "        'severity_default': 'medium',\n",
        "    },\n",
        "}\n",
        "\n",
        "POLICY_BUNDLES: Dict[str, List[str]] = {\n",
        "    'analytics': ['GDPR-PII-LOGGING', 'INT-PII-MASKING'],\n",
        "    'marketing_deck': ['MKT-CLAIMS-01', 'MKT-DISCLAIMER-01'],\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tool Definitions (FunctionTool)\n",
        "\n",
        "Each tool encapsulates a deterministic piece of the compliance workflow and is wrapped with `FunctionTool` for use by ADK agents.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "\n",
        "\n",
        "def detect_file_type_tool(text_or_code: str) -> Dict[str, Any]:\n",
        "    # Heuristically classify the input as Python, SQL, or deck text.\n",
        "    lowered = text_or_code.lower()\n",
        "    if 'select' in lowered and 'from' in lowered:\n",
        "        return {'file_type': 'code', 'code_language': 'sql', 'reason': 'SQL keywords detected'}\n",
        "    if 'def ' in text_or_code or 'import ' in text_or_code:\n",
        "        return {'file_type': 'code', 'code_language': 'python', 'reason': 'Python syntax detected'}\n",
        "    if 'slide 1' in lowered or 'slide 2' in lowered:\n",
        "        return {'file_type': 'deck', 'code_language': None, 'reason': 'Slide markers detected'}\n",
        "    return {'file_type': 'deck', 'code_language': None, 'reason': 'Defaulting to deck when code cues absent'}\n",
        "\n",
        "\n",
        "def select_policies_tool(context: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    # Return the policy bundle for the provided business context.\n",
        "    business_context = context.get('business_context', 'analytics')\n",
        "    rules = POLICY_BUNDLES.get(business_context, [])\n",
        "    explanation = f\"Selected {len(rules)} rules for context '{business_context}'.\"\n",
        "    return {'rules': rules, 'business_context': business_context, 'explanation': explanation}\n",
        "\n",
        "\n",
        "def scan_code_for_violations_tool(code: str, rules: List[str]) -> Dict[str, Any]:\n",
        "    # Scan Python or SQL code for simple privacy red flags.\n",
        "    findings: List[Dict[str, Any]] = []\n",
        "    lines = code.splitlines()\n",
        "    pii_patterns = [r'[\\w.-]+@[\\w.-]+', r'\b\\d{3}-\\d{2}-\\d{4}\b', r'phone', r'ssn', r'email']\n",
        "    for idx, line in enumerate(lines, start=1):\n",
        "        lowered = line.lower()\n",
        "        if any(re.search(pat, line, re.IGNORECASE) for pat in pii_patterns):\n",
        "            findings.append({\n",
        "                'id': f'F{len(findings)+1}',\n",
        "                'kind': 'PII_LOGGED',\n",
        "                'location': f'line {idx}',\n",
        "                'snippet': line.strip(),\n",
        "            })\n",
        "        if 'requests.post' in lowered or ('http' in lowered and 'requests' in lowered):\n",
        "            findings.append({\n",
        "                'id': f'F{len(findings)+1}',\n",
        "                'kind': 'DATA_EXPORT',\n",
        "                'location': f'line {idx}',\n",
        "                'snippet': line.strip(),\n",
        "            })\n",
        "        if 'logger' in lowered and ('email' in lowered or 'phone' in lowered or 'ssn' in lowered):\n",
        "            findings.append({\n",
        "                'id': f'F{len(findings)+1}',\n",
        "                'kind': 'PII_LOGGED',\n",
        "                'location': f'line {idx}',\n",
        "                'snippet': line.strip(),\n",
        "            })\n",
        "    return {'findings': findings}\n",
        "\n",
        "\n",
        "def scan_deck_for_violations_tool(deck_text: str, rules: List[str]) -> Dict[str, Any]:\n",
        "    # Scan deck-style text for risky marketing claims or missing disclaimers.\n",
        "    findings: List[Dict[str, Any]] = []\n",
        "    slides = [s.strip() for s in deck_text.split('Slide') if s.strip()]\n",
        "    for slide in slides:\n",
        "        parts = slide.split(':', 1)\n",
        "        slide_id = parts[0].strip() if parts else 'unknown'\n",
        "        content = parts[1] if len(parts) > 1 else slide\n",
        "        lowered = content.lower()\n",
        "        if 'guaranteed return' in lowered or 'risk-free' in lowered:\n",
        "            findings.append({\n",
        "                'id': f'F{len(findings)+1}',\n",
        "                'kind': 'UNAPPROVED_CLAIM',\n",
        "                'location': f'slide {slide_id}',\n",
        "                'snippet': content.strip(),\n",
        "            })\n",
        "    if 'MKT-DISCLAIMER-01' in rules:\n",
        "        has_disclaimer = any('disclaimer' in s.lower() for s in slides)\n",
        "        if not has_disclaimer:\n",
        "            findings.append({\n",
        "                'id': f'F{len(findings)+1}',\n",
        "                'kind': 'MISSING_DISCLAIMER',\n",
        "                'location': 'deck',\n",
        "                'snippet': 'No disclaimer slide found',\n",
        "            })\n",
        "    return {'findings': findings}\n",
        "\n",
        "\n",
        "def map_findings_to_rules_tool(\n",
        "    findings: List[Dict[str, Any]],\n",
        "    rules: List[str],\n",
        "    file_type: str,\n",
        "    code_language: Optional[str] = None,\n",
        "    business_context: Optional[str] = None,\n",
        ") -> Dict[str, Any]:\n",
        "    # Attach rule metadata and compute an aggregate risk score.\n",
        "    def pick_rule(finding_kind: str) -> str:\n",
        "        if finding_kind in {'PII_LOGGED', 'DATA_EXPORT'}:\n",
        "            return 'GDPR-PII-LOGGING' if 'GDPR-PII-LOGGING' in rules else (rules[0] if rules else 'UNKNOWN')\n",
        "        if finding_kind == 'UNAPPROVED_CLAIM':\n",
        "            return 'MKT-CLAIMS-01'\n",
        "        if finding_kind == 'MISSING_DISCLAIMER':\n",
        "            return 'MKT-DISCLAIMER-01'\n",
        "        return rules[0] if rules else 'UNKNOWN'\n",
        "\n",
        "    enriched = []\n",
        "    severity_weight = {'high': 3, 'medium': 2, 'low': 1}\n",
        "    total_weight = 0\n",
        "    for finding in findings:\n",
        "        rule_id = pick_rule(finding.get('kind', ''))\n",
        "        rule = REGULATION_KB.get(rule_id, {})\n",
        "        severity = rule.get('severity_default', 'medium')\n",
        "        total_weight += severity_weight.get(severity, 1)\n",
        "        enriched.append({\n",
        "            **finding,\n",
        "            'rule_id': rule_id,\n",
        "            'rule_name': rule.get('name', 'Unknown rule'),\n",
        "            'severity': severity,\n",
        "            'rationale': f\"Finding '{finding.get('kind')}' maps to policy '{rule_id}'.\",\n",
        "        })\n",
        "\n",
        "    score = min(100, total_weight * 15)\n",
        "    overall_risk = 'low'\n",
        "    if score >= 60:\n",
        "        overall_risk = 'high'\n",
        "    elif score >= 30:\n",
        "        overall_risk = 'medium'\n",
        "\n",
        "    return {\n",
        "        'file_type': file_type,\n",
        "        'code_language': code_language,\n",
        "        'business_context': business_context,\n",
        "        'rules_applied': rules,\n",
        "        'findings': enriched,\n",
        "        'overall_score': score,\n",
        "        'overall_risk': overall_risk,\n",
        "    }\n",
        "\n",
        "\n",
        "def render_compliance_report_tool(result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    # Render Markdown and JSON reports from the mapped findings.\n",
        "    analysis = result.get('analysis', result)\n",
        "    policy_note = result.get('policy_explanation', '')\n",
        "    timestamp = datetime.utcnow().isoformat() + 'Z'\n",
        "\n",
        "    header = f'# Compliance Report\n",
        "Generated: {timestamp}\n",
        "\n",
        "'\n",
        "    overview_lines = [\n",
        "        f\"- File type: **{analysis.get('file_type')}**\",\n",
        "        f\"- Code language: **{analysis.get('code_language') or 'N/A'}**\",\n",
        "        f\"- Business context: **{analysis.get('business_context') or 'unspecified'}**\",\n",
        "        f\"- Policies applied: {', '.join(analysis.get('rules_applied', [])) or 'None'}\",\n",
        "        f\"- Policy selection note: {policy_note or 'N/A'}\",\n",
        "        f\"- Overall risk: **{analysis.get('overall_risk')}** (score={analysis.get('overall_score')})\",\n",
        "    ]\n",
        "\n",
        "    summary_rows = []\n",
        "    for f in analysis.get('findings', []):\n",
        "        summary_rows.append(\n",
        "            f\"| {f.get('rule_id')} | {f.get('severity')} | {f.get('location')} | {f.get('snippet')} |\"\n",
        "        )\n",
        "    summary_table = '\n",
        "'.join([\n",
        "        '| Rule | Severity | Location | Snippet |',\n",
        "        '| --- | --- | --- | --- |',\n",
        "        *summary_rows,\n",
        "    ]) if summary_rows else 'No violations detected.'\n",
        "\n",
        "    detail_blocks = []\n",
        "    for f in analysis.get('findings', []):\n",
        "        rule = REGULATION_KB.get(f.get('rule_id'), {})\n",
        "        detail_blocks.append(\n",
        "            '\n",
        "'.join([\n",
        "                f\"### Finding {f.get('id')} \u2014 {f.get('kind')}\",\n",
        "                f\"- **Rule:** {f.get('rule_id')} \u2014 {rule.get('name', 'Unknown')}\",\n",
        "                f\"- **Severity:** {f.get('severity')}\",\n",
        "                f\"- **Location:** {f.get('location')}\",\n",
        "                f\"- **Snippet:** `{f.get('snippet')}`\",\n",
        "                f\"- **Description:** {rule.get('description', 'No description')}\",\n",
        "                f\"- **Recommended remediation:** Align logging/export with {rule.get('name', 'the rule')} and mask PII.\",\n",
        "            ])\n",
        "        )\n",
        "\n",
        "    report_markdown = '\n",
        "\n",
        "'.join([\n",
        "        header,\n",
        "        '## Overview',\n",
        "        '\n",
        "'.join(overview_lines),\n",
        "        '## Summary of Violations',\n",
        "        summary_table,\n",
        "        '## Detailed Findings',\n",
        "        '\n",
        "\n",
        "'.join(detail_blocks) if detail_blocks else 'No detailed findings.',\n",
        "    ])\n",
        "\n",
        "    report_json = {\n",
        "        'generated_at': timestamp,\n",
        "        'analysis': analysis,\n",
        "        'policy_explanation': policy_note,\n",
        "    }\n",
        "\n",
        "    return {'report_markdown': report_markdown, 'report_json': report_json}\n",
        "\n",
        "\n",
        "def evaluate_on_test_cases_tool(test_cases: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    # Run the pipeline on synthetic cases and compute simple metrics.\n",
        "    results = []\n",
        "    correct_any = 0\n",
        "    total = len(test_cases)\n",
        "    mae = 0.0\n",
        "    for case in test_cases:\n",
        "        context = case.get('business_context', 'analytics')\n",
        "        detection = detect_file_type_tool(case['input_text'])\n",
        "        policies = select_policies_tool({'business_context': context})\n",
        "        if detection['file_type'] == 'code':\n",
        "            findings = scan_code_for_violations_tool(case['input_text'], policies['rules'])['findings']\n",
        "        else:\n",
        "            findings = scan_deck_for_violations_tool(case['input_text'], policies['rules'])['findings']\n",
        "        map_findings_to_rules_tool(\n",
        "            findings=findings,\n",
        "            rules=policies['rules'],\n",
        "            file_type=detection['file_type'],\n",
        "            code_language=detection.get('code_language'),\n",
        "            business_context=context,\n",
        "        )\n",
        "        results.append({'case': case.get('name', 'case'), 'predicted': len(findings), 'expected': case.get('expected_violation_count', 0)})\n",
        "        mae += abs(len(findings) - case.get('expected_violation_count', 0))\n",
        "        if (len(findings) > 0) == (case.get('expected_violation_count', 0) > 0):\n",
        "            correct_any += 1\n",
        "    mae = mae / total if total else 0\n",
        "    accuracy = correct_any / total if total else 0\n",
        "    explanation = f'Evaluated {total} cases: MAE={mae:.2f}, accuracy(any_violation)={accuracy:.2%}.'\n",
        "    return {'case_results': results, 'mae': mae, 'accuracy_any': accuracy, 'explanation': explanation}\n",
        "\n",
        "\n",
        "DETECT_FILE_TYPE = FunctionTool(fn=detect_file_type_tool, name='detect_file_type_tool')\n",
        "SELECT_POLICIES = FunctionTool(fn=select_policies_tool, name='select_policies_tool')\n",
        "SCAN_CODE = FunctionTool(fn=scan_code_for_violations_tool, name='scan_code_for_violations_tool')\n",
        "SCAN_DECK = FunctionTool(fn=scan_deck_for_violations_tool, name='scan_deck_for_violations_tool')\n",
        "MAP_FINDINGS = FunctionTool(fn=map_findings_to_rules_tool, name='map_findings_to_rules_tool')\n",
        "RENDER_REPORT = FunctionTool(fn=render_compliance_report_tool, name='render_compliance_report_tool')\n",
        "EVALUATE_CASES = FunctionTool(fn=evaluate_on_test_cases_tool, name='evaluate_on_test_cases_tool')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agent Definitions\n",
        "\n",
        "The ADK agents are declared to mirror the multi-agent architecture. For offline execution we drive the tools directly, but these agent objects illustrate how orchestration, delegation, and reporting could be coordinated in an ADK workflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "MODEL_NAME = 'gemini-2.5-flash'\n",
        "\n",
        "orchestrator_agent = Agent(\n",
        "    name='orchestrator',\n",
        "    model=MODEL_NAME,\n",
        "    instructions=(\n",
        "        'You are the orchestration agent. Given user input you classify the artifact, '\n",
        "        'select policies, delegate to specialized agents, and consolidate their outputs into a compliance report. '\n",
        "        'Prefer calling tools and sub-agents over free-form answers.'\n",
        "    ),\n",
        "    tools=[DETECT_FILE_TYPE, SELECT_POLICIES, RENDER_REPORT],\n",
        ")\n",
        "\n",
        "code_compliance_agent = Agent(\n",
        "    name='code_compliance_agent',\n",
        "    model=MODEL_NAME,\n",
        "    instructions=(\n",
        "        'You analyze analytics code (Python or SQL) for policy and regulatory risks. '\n",
        "        'Use the scanning tools and return only tool outputs.'\n",
        "    ),\n",
        "    tools=[SCAN_CODE, MAP_FINDINGS],\n",
        ")\n",
        "\n",
        "deck_compliance_agent = Agent(\n",
        "    name='deck_compliance_agent',\n",
        "    model=MODEL_NAME,\n",
        "    instructions=(\n",
        "        'You analyze business decks for risky claims and missing disclaimers using tools only.'\n",
        "    ),\n",
        "    tools=[SCAN_DECK, MAP_FINDINGS],\n",
        ")\n",
        "\n",
        "evaluation_agent = Agent(\n",
        "    name='evaluation_agent',\n",
        "    model=MODEL_NAME,\n",
        "    instructions=(\n",
        "        'You run evaluation suites on the compliance system using the provided tool, summarizing metrics only with tool outputs.'\n",
        "    ),\n",
        "    tools=[EVALUATE_CASES],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Service Wrapper Using a Session\n",
        "\n",
        "The helper below drives the end-to-end pipeline within a lightweight `Session`, showing how state can be reused across runs in the same notebook session.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "\n",
        "\n",
        "def run_compliance_service(input_text: str, business_context: str = 'analytics') -> Dict[str, Any]:\n",
        "    # Run the orchestrated compliance review and return Markdown plus JSON artifacts.\n",
        "    session = Session()\n",
        "\n",
        "    detection = detect_file_type_tool(input_text)\n",
        "    policies = select_policies_tool({'business_context': business_context})\n",
        "\n",
        "    if detection['file_type'] == 'code':\n",
        "        raw_findings = scan_code_for_violations_tool(input_text, policies['rules']).get('findings', [])\n",
        "    else:\n",
        "        raw_findings = scan_deck_for_violations_tool(input_text, policies['rules']).get('findings', [])\n",
        "\n",
        "    mapped = map_findings_to_rules_tool(\n",
        "        findings=raw_findings,\n",
        "        rules=policies['rules'],\n",
        "        file_type=detection['file_type'],\n",
        "        code_language=detection.get('code_language'),\n",
        "        business_context=policies.get('business_context'),\n",
        "    )\n",
        "\n",
        "    report_payload = {'analysis': mapped, 'policy_explanation': policies.get('explanation')}\n",
        "    rendered = render_compliance_report_tool(report_payload)\n",
        "\n",
        "    session.last_report = rendered\n",
        "    return rendered\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demo: Running the Multi-Agent Compliance Service\n",
        "\n",
        "We exercise the pipeline on two synthetic artifacts: a Python analytics snippet and a marketing deck with intentional compliance issues.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Example 1: Python analytics code with PII logging and an external export\n",
        "analytics_code = '''\n",
        "import requests\n",
        "\n",
        "def send_report(user_email, score):\n",
        "    logger.info(f\"user_email={user_email}\")\n",
        "    requests.post('http://external-collector.example.com', json={'email': user_email, 'score': score})\n",
        "    return 'sent'\n",
        "'''\n",
        "\n",
        "analytics_report = run_compliance_service(analytics_code, business_context='analytics')\n",
        "print(json.dumps(analytics_report['report_json'], indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Display Markdown report for analytics example (rendering handled by notebooks)\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "display(Markdown(analytics_report['report_markdown']))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Example 2: Marketing deck text with unapproved claims and no disclaimer\n",
        "marketing_deck = '''\n",
        "Slide 1: Product launch overview\n",
        "Slide 2: Our customers enjoy guaranteed returns with zero risk\n",
        "Slide 3: Performance metrics from Q1\n",
        "'''\n",
        "\n",
        "marketing_report = run_compliance_service(marketing_deck, business_context='marketing_deck')\n",
        "print(json.dumps(marketing_report['report_json'], indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Display Markdown report for marketing example\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "display(Markdown(marketing_report['report_markdown']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation & Discussion\n",
        "\n",
        "A handful of synthetic cases are used to illustrate how evaluation tooling can summarize performance of the end-to-end compliance flow.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "TEST_CASES = [\n",
        "    {\n",
        "        'name': 'clean_python',\n",
        "        'input_text': \"def add(a, b):\n",
        "    return a + b\",\n",
        "        'expected_violation_count': 0,\n",
        "        'business_context': 'analytics',\n",
        "    },\n",
        "    {\n",
        "        'name': 'pii_in_logs',\n",
        "        'input_text': \"logger.info('email=alice@example.com')\",\n",
        "        'expected_violation_count': 1,\n",
        "        'business_context': 'analytics',\n",
        "    },\n",
        "    {\n",
        "        'name': 'marketing_claims',\n",
        "        'input_text': 'Slide 1: guaranteed returns for all customers',\n",
        "        'expected_violation_count': 2,\n",
        "        'business_context': 'marketing_deck',\n",
        "    },\n",
        "    {\n",
        "        'name': 'disclaimer_present',\n",
        "        'input_text': 'Slide 1: Metrics\n",
        "Slide 2: Disclaimer: results vary',\n",
        "        'expected_violation_count': 0,\n",
        "        'business_context': 'marketing_deck',\n",
        "    },\n",
        "]\n",
        "\n",
        "eval_results = evaluate_on_test_cases_tool(TEST_CASES)\n",
        "print(json.dumps(eval_results, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The simple metrics above capture mean absolute error for predicted violation counts and accuracy for whether any violation was detected. Because the rule set and detection heuristics are intentionally lightweight, the evaluation is illustrative rather than definitive.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}