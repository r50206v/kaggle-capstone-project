{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple Multi-Agent Compliance Report Service",
        "",
        "This notebook demonstrates a local, end-to-end multi-agent workflow for generating standardized compliance reports using Google ADK. Everything stays within the Kaggle environment with no external network calls beyond package installation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup",
        "Install required packages and import common utilities plus the Google ADK components used across the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Install dependencies (kept in the notebook so it works in fresh Kaggle runtimes)\n",
        "!pip install -q google-adk PyPDF2\n",
        "\n",
        "import json\n",
        "from typing import Dict, Any, List\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "from google.adk.agents import Agent\n",
        "from google.adk.tools.function_tool import FunctionTool\n",
        "from google.adk.sessions import Session\n",
        "\n",
        "MODEL_NAME = \"gemini-2.5-flash\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Project Overview",
        "",
        "Goal: **Demonstrate a simple, locally hosted multi-agent service that generates standardized compliance reports for either code files or PDF business decks, using Google ADK with multiple agents, tools, sessions, and long-term memory.**",
        "",
        "The five agents and their roles are:",
        "- **Reception Agent:** Collects company and file information, classifies the file, and routes processing.",
        "- **PDF Processor Agent:** Reads PDF-like files and extracts simple metrics.",
        "- **Code Processor Agent:** Reads code files and extracts simple metrics.",
        "- **Report Generator Agent:** Produces structured report drafts using analysis metrics and any evaluator feedback.",
        "- **Evaluating Agent:** Critiques draft reports and suggests improvements until they look mature.",
        "",
        "All tools accept JSON **strings** as input and return JSON-formatted **strings** to keep I/O consistent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Long-term Memory & Helper Data Structures",
        "We maintain a lightweight long-term memory to demonstrate persistence across runs. Helper functions manage memory entries and a tiny supported companies list keeps examples grounded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Simple long-term memory for demo purposes\n",
        "LONG_TERM_MEMORY: Dict[str, List[Dict[str, Any]]] = {\n",
        "    \"reports\": []\n",
        "}\n",
        "\n",
        "SUPPORTED_COMPANIES = [\"DemoCorp\", \"AcmeAnalytics\", \"Globex\"]\n",
        "\n",
        "\n",
        "def add_report_to_memory(company: str, file_path: str, report: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"Append a compact report summary to long-term memory.\"\"\"\n",
        "    LONG_TERM_MEMORY[\"reports\"].append(\n",
        "        {\n",
        "            \"company\": company,\n",
        "            \"file_path\": str(Path(file_path)),\n",
        "            \"timestamp\": datetime.utcnow().isoformat(),\n",
        "            \"summary\": report.get(\"summary\", \"\"),\n",
        "            \"risk_score\": report.get(\"risk_score\", 0),\n",
        "        }\n",
        "    )\n",
        "    return {\n",
        "        \"status\": \"saved\",\n",
        "        \"total_reports\": len(LONG_TERM_MEMORY[\"reports\"]),\n",
        "    }\n",
        "\n",
        "\n",
        "def lookup_reports(company: str, file_path: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Return prior reports for the same company or file if present.\"\"\"\n",
        "    normalized_path = str(Path(file_path))\n",
        "    return [\n",
        "        r\n",
        "        for r in LONG_TERM_MEMORY[\"reports\"]\n",
        "        if r.get(\"company\") == company or r.get(\"file_path\") == normalized_path\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Tool Implementations (string in -> JSON string out)",
        "Each tool immediately parses the JSON string payload and always returns JSON-encoded strings. They are wrapped with `FunctionTool` so Google ADK agents can call them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Tool 1: File info and routing helper\n",
        "\n",
        "def file_info_tool_fn(payload: str) -> str:\n",
        "    \"\"\"Classify a file path as code or pdf-like and normalize paths.\"\"\"\n",
        "    data = json.loads(payload)\n",
        "    company = data.get(\"company\", \"UnknownCo\")\n",
        "    file_path = Path(data.get(\"file_path\", \"\"))\n",
        "    suffix = file_path.suffix.lower()\n",
        "    file_type = \"pdf\" if suffix == \".pdf\" else \"code\"\n",
        "    result = {\n",
        "        \"company\": company,\n",
        "        \"file_path\": str(file_path),\n",
        "        \"file_type\": file_type,\n",
        "    }\n",
        "    return json.dumps(result)\n",
        "\n",
        "\n",
        "# Tool 2: PDF reader\n",
        "\n",
        "def pdf_reader_tool_fn(payload: str) -> str:\n",
        "    \"\"\"Read PDF contents; fall back to placeholder text if reading fails.\"\"\"\n",
        "    data = json.loads(payload)\n",
        "    file_path = Path(data.get(\"file_path\", \"\"))\n",
        "    extracted_text = \"\"\n",
        "    page_count = 0\n",
        "    if file_path.exists() and file_path.suffix.lower() == \".pdf\":\n",
        "        try:\n",
        "            from PyPDF2 import PdfReader\n",
        "\n",
        "            reader = PdfReader(str(file_path))\n",
        "            page_count = len(reader.pages)\n",
        "            extracted_text = \"\n",
        "\".join(page.extract_text() or \"\" for page in reader.pages)\n",
        "        except Exception:\n",
        "            extracted_text = \"Example PDF content fallback.\"\n",
        "    else:\n",
        "        extracted_text = file_path.read_text() if file_path.exists() else \"Example PDF content fallback.\"\n",
        "        page_count = max(1, extracted_text.count(\"Slide\"))\n",
        "    result = {\n",
        "        \"file_path\": str(file_path),\n",
        "        \"extracted_text\": extracted_text or \"Example PDF content fallback.\",\n",
        "        \"page_count\": page_count or 1,\n",
        "    }\n",
        "    return json.dumps(result)\n",
        "\n",
        "\n",
        "# Tool 3: Code reader\n",
        "\n",
        "def code_reader_tool_fn(payload: str) -> str:\n",
        "    \"\"\"Load code text with naive language detection.\"\"\"\n",
        "    data = json.loads(payload)\n",
        "    file_path = Path(data.get(\"file_path\", \"\"))\n",
        "    language = \"unknown\"\n",
        "    if file_path.suffix.lower() == \".py\":\n",
        "        language = \"python\"\n",
        "    elif file_path.suffix.lower() == \".sql\":\n",
        "        language = \"sql\"\n",
        "    code_text = file_path.read_text() if file_path.exists() else f\"Placeholder snippet for {file_path.name}\"\n",
        "    result = {\n",
        "        \"file_path\": str(file_path),\n",
        "        \"language\": language,\n",
        "        \"code_text\": code_text,\n",
        "    }\n",
        "    return json.dumps(result)\n",
        "\n",
        "\n",
        "# Tool 4: PDF analysis\n",
        "\n",
        "def pdf_analysis_tool_fn(payload: str) -> str:\n",
        "    \"\"\"Heuristically analyze PDF-like text for risk signals.\"\"\"\n",
        "    data = json.loads(payload)\n",
        "    company = data.get(\"company\", \"UnknownCo\")\n",
        "    file_path = data.get(\"file_path\", \"\")\n",
        "    extracted_text = data.get(\"extracted_text\", \"\")\n",
        "    lines = extracted_text.splitlines()\n",
        "    estimated_slides = sum(1 for line in lines if \"slide\" in line.lower()) or max(1, len(lines))\n",
        "    risky_terms = [term for term in [\"guarantee\", \"confidential\", \"secret\"] if term in extracted_text.lower()]\n",
        "    contains_confidential = any(term in extracted_text.lower() for term in [\"confidential\", \"secret\"])\n",
        "    result = {\n",
        "        \"file_path\": file_path,\n",
        "        \"company\": company,\n",
        "        \"metrics\": {\n",
        "            \"estimated_slides\": estimated_slides,\n",
        "            \"contains_confidential\": contains_confidential,\n",
        "            \"risky_terms\": risky_terms,\n",
        "        },\n",
        "    }\n",
        "    return json.dumps(result)\n",
        "\n",
        "\n",
        "# Tool 5: Code analysis\n",
        "\n",
        "def code_analysis_tool_fn(payload: str) -> str:\n",
        "    \"\"\"Heuristically scan code for PII-like terms and logging usage.\"\"\"\n",
        "    data = json.loads(payload)\n",
        "    company = data.get(\"company\", \"UnknownCo\")\n",
        "    file_path = data.get(\"file_path\", \"\")\n",
        "    code_text = data.get(\"code_text\", \"\")\n",
        "    pii_terms = [\"email\", \"ssn\", \"phone\", \"customer\"]\n",
        "    found_pii = [term for term in pii_terms if term.lower() in code_text.lower()]\n",
        "    log_statement_count = code_text.count(\"print(\") + code_text.count(\"logger.\") + code_text.count(\"logging.\")\n",
        "    total_lines = len(code_text.splitlines()) if code_text else 0\n",
        "    result = {\n",
        "        \"file_path\": file_path,\n",
        "        \"company\": company,\n",
        "        \"metrics\": {\n",
        "            \"pii_mentions\": found_pii,\n",
        "            \"log_statement_count\": log_statement_count,\n",
        "            \"total_lines\": total_lines,\n",
        "        },\n",
        "    }\n",
        "    return json.dumps(result)\n",
        "\n",
        "\n",
        "# Tool 6: Report drafting\n",
        "\n",
        "def report_draft_tool_fn(payload: str) -> str:\n",
        "    \"\"\"Generate a structured compliance report draft from metrics.\"\"\"\n",
        "    data = json.loads(payload)\n",
        "    company = data.get(\"company\", \"UnknownCo\")\n",
        "    file_path = data.get(\"file_path\", \"\")\n",
        "    file_type = data.get(\"file_type\", \"unknown\")\n",
        "    analysis_metrics = data.get(\"analysis_metrics\", {})\n",
        "    previous_feedback = data.get(\"previous_feedback\", \"\")\n",
        "\n",
        "    risk_score = 20\n",
        "    if file_type == \"code\":\n",
        "        risk_score += 10 * len(analysis_metrics.get(\"pii_mentions\", []))\n",
        "        risk_score += 2 * analysis_metrics.get(\"log_statement_count\", 0)\n",
        "    else:\n",
        "        risk_score += 15 * len(analysis_metrics.get(\"risky_terms\", []))\n",
        "        risk_score += 5 if analysis_metrics.get(\"contains_confidential\") else 0\n",
        "    risk_score = min(100, risk_score)\n",
        "\n",
        "    summary = f\"Draft compliance report for {company} covering {file_type} file {file_path}.\"\n",
        "    if previous_feedback:\n",
        "        summary += f\" Incorporated feedback: {previous_feedback}.\"\n",
        "\n",
        "    sections = {\n",
        "        \"overview\": summary,\n",
        "        \"key_findings\": [],\n",
        "        \"recommendations\": [],\n",
        "    }\n",
        "\n",
        "    if file_type == \"code\":\n",
        "        sections[\"key_findings\"].append(\n",
        "            f\"PII mentions found: {', '.join(analysis_metrics.get('pii_mentions', [])) or 'none'}.\"\n",
        "        )\n",
        "        sections[\"key_findings\"].append(\n",
        "            f\"Logging statements counted: {analysis_metrics.get('log_statement_count', 0)}.\"\n",
        "        )\n",
        "        sections[\"recommendations\"].append(\"Review logging for sensitive data exposure.\")\n",
        "        if analysis_metrics.get(\"pii_mentions\"):\n",
        "            sections[\"recommendations\"].append(\"Apply masking or tokenization to PII fields.\")\n",
        "    else:\n",
        "        sections[\"key_findings\"].append(\n",
        "            f\"Estimated slides: {analysis_metrics.get('estimated_slides', 'unknown')}.\"\n",
        "        )\n",
        "        sections[\"key_findings\"].append(\n",
        "            f\"Confidential terms present: {analysis_metrics.get('contains_confidential', False)}.\"\n",
        "        )\n",
        "        sections[\"recommendations\"].append(\"Ensure disclaimers are present for guarantees and promises.\")\n",
        "        if analysis_metrics.get(\"risky_terms\"):\n",
        "            sections[\"recommendations\"].append(\"Highlight and qualify risky commitments in the deck.\")\n",
        "\n",
        "    report = {\n",
        "        \"company\": company,\n",
        "        \"file_path\": file_path,\n",
        "        \"file_type\": file_type,\n",
        "        \"summary\": summary,\n",
        "        \"risk_score\": risk_score,\n",
        "        \"sections\": sections,\n",
        "    }\n",
        "    return json.dumps(report)\n",
        "\n",
        "\n",
        "# Tool 7: Report critique\n",
        "\n",
        "def report_critique_tool_fn(payload: str) -> str:\n",
        "    \"\"\"Assess a draft report and suggest improvements.\"\"\"\n",
        "    data = json.loads(payload)\n",
        "    report = data.get(\"report\", {})\n",
        "    recommendations = report.get(\"sections\", {}).get(\"recommendations\", [])\n",
        "    risk_score = report.get(\"risk_score\")\n",
        "    missing_fields = []\n",
        "    if not recommendations:\n",
        "        missing_fields.append(\"recommendations\")\n",
        "    if risk_score is None:\n",
        "        missing_fields.append(\"risk_score\")\n",
        "\n",
        "    is_satisfactory = len(missing_fields) == 0\n",
        "    improvement_suggestions = []\n",
        "    if not is_satisfactory:\n",
        "        improvement_suggestions.append(\"Add actionable recommendations for the audience.\")\n",
        "    if risk_score is None:\n",
        "        improvement_suggestions.append(\"Compute a risk_score between 0 and 100.\")\n",
        "\n",
        "    overall_comment = \"Report looks solid.\" if is_satisfactory else \"Please address the listed issues.\"\n",
        "    result = {\n",
        "        \"is_satisfactory\": is_satisfactory,\n",
        "        \"overall_comment\": overall_comment,\n",
        "        \"improvement_suggestions\": improvement_suggestions,\n",
        "    }\n",
        "    return json.dumps(result)\n",
        "\n",
        "\n",
        "# Tool 8: Memory update\n",
        "\n",
        "def memory_update_tool_fn(payload: str) -> str:\n",
        "    \"\"\"Persist a compact report summary to the in-notebook memory store.\"\"\"\n",
        "    data = json.loads(payload)\n",
        "    company = data.get(\"company\", \"UnknownCo\")\n",
        "    file_path = data.get(\"file_path\", \"\")\n",
        "    report = data.get(\"report\", {})\n",
        "    result = add_report_to_memory(company, file_path, report)\n",
        "    return json.dumps(result)\n",
        "\n",
        "\n",
        "# Tool 9: Memory lookup\n",
        "\n",
        "def memory_lookup_tool_fn(payload: str) -> str:\n",
        "    \"\"\"Fetch previously saved reports for the same company or file path.\"\"\"\n",
        "    data = json.loads(payload)\n",
        "    company = data.get(\"company\", \"UnknownCo\")\n",
        "    file_path = data.get(\"file_path\", \"\")\n",
        "    previous_reports = lookup_reports(company, file_path)\n",
        "    return json.dumps({\"previous_reports\": previous_reports})\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Wrap tool functions with FunctionTool for ADK agents\n",
        "file_info_tool = FunctionTool.from_fn(\n",
        "    file_info_tool_fn,\n",
        "    name=\"file_info_tool\",\n",
        "    description=\"Classify files and normalize paths before routing.\",\n",
        ")\n",
        "pdf_reader_tool = FunctionTool.from_fn(\n",
        "    pdf_reader_tool_fn,\n",
        "    name=\"pdf_reader_tool\",\n",
        "    description=\"Read PDF-like files and return extracted text.\",\n",
        ")\n",
        "code_reader_tool = FunctionTool.from_fn(\n",
        "    code_reader_tool_fn,\n",
        "    name=\"code_reader_tool\",\n",
        "    description=\"Read code files with naive language detection.\",\n",
        ")\n",
        "pdf_analysis_tool = FunctionTool.from_fn(\n",
        "    pdf_analysis_tool_fn,\n",
        "    name=\"pdf_analysis_tool\",\n",
        "    description=\"Heuristically analyze PDF text for risk signals.\",\n",
        ")\n",
        "code_analysis_tool = FunctionTool.from_fn(\n",
        "    code_analysis_tool_fn,\n",
        "    name=\"code_analysis_tool\",\n",
        "    description=\"Heuristically analyze code for PII hints and logging.\",\n",
        ")\n",
        "report_draft_tool = FunctionTool.from_fn(\n",
        "    report_draft_tool_fn,\n",
        "    name=\"report_draft_tool\",\n",
        "    description=\"Build a structured compliance report draft.\",\n",
        ")\n",
        "report_critique_tool = FunctionTool.from_fn(\n",
        "    report_critique_tool_fn,\n",
        "    name=\"report_critique_tool\",\n",
        "    description=\"Critique a draft report and request fixes if needed.\",\n",
        ")\n",
        "memory_update_tool = FunctionTool.from_fn(\n",
        "    memory_update_tool_fn,\n",
        "    name=\"memory_update_tool\",\n",
        "    description=\"Store reports in long-term memory.\",\n",
        ")\n",
        "memory_lookup_tool = FunctionTool.from_fn(\n",
        "    memory_lookup_tool_fn,\n",
        "    name=\"memory_lookup_tool\",\n",
        "    description=\"Retrieve prior reports for a company or file.\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Agent Definitions (5 agents)",
        "Agents use the tools above to collaborate on compliance reports. Each agent keeps instructions concise and tool-focused so their outputs remain deterministic for the demo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Reception agent to classify and route requests\n",
        "reception_agent = Agent(\n",
        "    name=\"reception_agent\",\n",
        "    model=MODEL_NAME,\n",
        "    instructions=(\n",
        "        \"You are the reception and routing agent. Given a user message that includes \"\n",
        "        \"a company name and a file path, you call tools to understand the file and \"\n",
        "        \"then decide whether to send it to the PDF processor or the Code processor. \"\n",
        "        \"Always use the tools and return a concise JSON-like routing summary.\"\n",
        "    ),\n",
        "    tools=[file_info_tool, memory_lookup_tool],\n",
        ")\n",
        "\n",
        "# PDF processor agent\n",
        "pdf_processor_agent = Agent(\n",
        "    name=\"pdf_processor_agent\",\n",
        "    model=MODEL_NAME,\n",
        "    instructions=(\n",
        "        \"You process PDF business decks. You use tools to read the PDF and extract \"\n",
        "        \"simple metrics that will be used later in a compliance report. You never make \"\n",
        "        \"up metrics; you only summarize tool output.\"\n",
        "    ),\n",
        "    tools=[pdf_reader_tool, pdf_analysis_tool],\n",
        ")\n",
        "\n",
        "# Code processor agent\n",
        "code_processor_agent = Agent(\n",
        "    name=\"code_processor_agent\",\n",
        "    model=MODEL_NAME,\n",
        "    instructions=(\n",
        "        \"You process analytics code files (Python, SQL, etc.). Use tools to read the \"\n",
        "        \"code and compute simple metrics like PII mentions and logging counts for later \"\n",
        "        \"reporting.\"\n",
        "    ),\n",
        "    tools=[code_reader_tool, code_analysis_tool],\n",
        ")\n",
        "\n",
        "# Report generator agent\n",
        "report_generator_agent = Agent(\n",
        "    name=\"report_generator_agent\",\n",
        "    model=MODEL_NAME,\n",
        "    instructions=(\n",
        "        \"You generate a standardized compliance report in structured JSON. You call \"\n",
        "        \"tools to transform analysis metrics and any previous feedback into a clearer, \"\n",
        "        \"more complete report.\"\n",
        "    ),\n",
        "    tools=[report_draft_tool],\n",
        ")\n",
        "\n",
        "# Evaluating agent\n",
        "\n",
        "evaluating_agent = Agent(\n",
        "    name=\"evaluating_agent\",\n",
        "    model=MODEL_NAME,\n",
        "    instructions=(\n",
        "        \"You evaluate draft compliance reports. You use tools to critique the report \"\n",
        "        \"and decide if it is satisfactory. If not, you provide clear suggestions for \"\n",
        "        \"the report generator to improve it. Stop once the report is satisfactory or \"\n",
        "        \"after a small number of iterations.\"\n",
        "    ),\n",
        "    tools=[report_critique_tool, memory_update_tool],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Orchestration / Service Function",
        "The helper below wires the agents together, demonstrates ADK sessions, and runs a simple critique loop between the report generator and evaluator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def run_compliance_service(\n",
        "    user_message: str,\n",
        "    company: str,\n",
        "    file_path: str,\n",
        "    max_iterations: int = 3,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"End-to-end local compliance service orchestration.\"\"\"\n",
        "    session = Session()\n",
        "    critique_history: List[Dict[str, Any]] = []\n",
        "\n",
        "    # 1) Reception: classify the file\n",
        "    reception_prompt = (\n",
        "        f\"User message: {user_message}. Company: {company}. File path: {file_path}. \"\n",
        "        \"Use tools to classify the file and return JSON with company, file_path, and file_type.\"\n",
        "    )\n",
        "    reception_result = reception_agent.run(reception_prompt, session=session)\n",
        "    routing_output = getattr(reception_result, \"output\", reception_result)\n",
        "    try:\n",
        "        routing_dict = json.loads(routing_output) if isinstance(routing_output, str) else routing_output\n",
        "    except Exception:\n",
        "        routing_payload = json.dumps({\"company\": company, \"file_path\": file_path})\n",
        "        routing_dict = json.loads(file_info_tool_fn(routing_payload))\n",
        "\n",
        "    file_type = routing_dict.get(\"file_type\", \"code\")\n",
        "    normalized_path = routing_dict.get(\"file_path\", file_path)\n",
        "\n",
        "    # 2) Dispatch to processor agents\n",
        "    if file_type == \"pdf\":\n",
        "        reader_payload = json.dumps({\"file_path\": normalized_path})\n",
        "        pdf_text = json.loads(pdf_reader_tool_fn(reader_payload))\n",
        "        analysis_payload = json.dumps(\n",
        "            {\n",
        "                \"company\": company,\n",
        "                \"file_path\": normalized_path,\n",
        "                \"extracted_text\": pdf_text.get(\"extracted_text\", \"\"),\n",
        "            }\n",
        "        )\n",
        "        analysis = json.loads(pdf_analysis_tool_fn(analysis_payload))\n",
        "        metrics = analysis.get(\"metrics\", {})\n",
        "    else:\n",
        "        reader_payload = json.dumps({\"file_path\": normalized_path})\n",
        "        code_text = json.loads(code_reader_tool_fn(reader_payload))\n",
        "        analysis_payload = json.dumps(\n",
        "            {\n",
        "                \"company\": company,\n",
        "                \"file_path\": normalized_path,\n",
        "                \"code_text\": code_text.get(\"code_text\", \"\"),\n",
        "            }\n",
        "        )\n",
        "        analysis = json.loads(code_analysis_tool_fn(analysis_payload))\n",
        "        metrics = analysis.get(\"metrics\", {})\n",
        "\n",
        "    # 3) Critique loop between generator and evaluator\n",
        "    previous_feedback = \"\"\n",
        "    final_report: Dict[str, Any] = {}\n",
        "\n",
        "    for iteration in range(1, max_iterations + 1):\n",
        "        draft_payload = json.dumps(\n",
        "            {\n",
        "                \"company\": company,\n",
        "                \"file_path\": normalized_path,\n",
        "                \"file_type\": file_type,\n",
        "                \"analysis_metrics\": metrics,\n",
        "                \"previous_feedback\": previous_feedback,\n",
        "            }\n",
        "        )\n",
        "        draft_report = json.loads(report_draft_tool_fn(draft_payload))\n",
        "\n",
        "        critique_payload = json.dumps(\n",
        "            {\n",
        "                \"report\": draft_report,\n",
        "                \"company\": company,\n",
        "                \"file_path\": normalized_path,\n",
        "            }\n",
        "        )\n",
        "        critique = json.loads(report_critique_tool_fn(critique_payload))\n",
        "        critique[\"iteration\"] = iteration\n",
        "        critique_history.append(critique)\n",
        "\n",
        "        if critique.get(\"is_satisfactory\"):\n",
        "            final_report = draft_report\n",
        "            break\n",
        "\n",
        "        previous_feedback = \"; \".join(critique.get(\"improvement_suggestions\", []))\n",
        "        final_report = draft_report\n",
        "\n",
        "    # 4) Save to long-term memory\n",
        "    memory_payload = json.dumps(\n",
        "        {\"company\": company, \"file_path\": normalized_path, \"report\": final_report}\n",
        "    )\n",
        "    memory_status = json.loads(memory_update_tool_fn(memory_payload))\n",
        "\n",
        "    return {\n",
        "        \"final_report\": final_report,\n",
        "        \"critique_history\": critique_history,\n",
        "        \"memory_status\": memory_status,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Demo Runs",
        "Create small sample artifacts and run the compliance service twice: once for code and once for a deck-like text file (treated as a PDF stand-in)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Create sample files\n",
        "Path(\"sample_code.py\").write_text(\n",
        "    \"import logging\n",
        "\n",
        "email = 'test@example.com'\n",
        "logging.info(f'user email: {email}')\n",
        "\"\n",
        ")\n",
        "Path(\"sample_deck.txt\").write_text(\n",
        "    \"Slide 1: Welcome\n",
        "Slide 2: We guarantee amazing returns.\n",
        "Slide 3: Thank you.\n",
        "\"\n",
        ")\n",
        "\n",
        "# Run the compliance service for both examples\n",
        "final_report_code = run_compliance_service(\n",
        "    user_message=\"Please review this analytics script.\",\n",
        "    company=\"DemoCorp\",\n",
        "    file_path=\"sample_code.py\",\n",
        ")\n",
        "\n",
        "final_report_deck = run_compliance_service(\n",
        "    user_message=\"Please review this business deck.\",\n",
        "    company=\"DemoCorp\",\n",
        "    file_path=\"sample_deck.txt\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pprint\n",
        "\n",
        "print(\"Final report for code:\")\n",
        "pprint.pp(final_report_code)\n",
        "print(\"\n",
        "Final report for deck-like file:\")\n",
        "pprint.pp(final_report_deck)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "\n",
        "def render_report_markdown(report_bundle: Dict[str, Any]) -> str:\n",
        "    \"\"\"Pretty render the final report dictionary into Markdown.\"\"\"\n",
        "    report = report_bundle.get(\"final_report\", {}) if \"final_report\" in report_bundle else report_bundle\n",
        "    sections = report.get(\"sections\", {})\n",
        "    key_findings = \"\n",
        "\".join(f\"- {item}\" for item in sections.get(\"key_findings\", []))\n",
        "    recommendations = \"\n",
        "\".join(f\"- {item}\" for item in sections.get(\"recommendations\", []))\n",
        "    md = f\"\"\"\n",
        "### Compliance Report for {report.get('company')} ({report.get('file_type')})\n",
        "*File:* {report.get('file_path')}  \n",
        "*Risk score:* {report.get('risk_score')}\n",
        "\n",
        "**Summary**  \n",
        "{report.get('summary')}\n",
        "\n",
        "**Key Findings**\n",
        "{key_findings or '- None recorded.'}\n",
        "\n",
        "**Recommendations**\n",
        "{recommendations or '- None recorded.'}\n",
        "\"\"\"\n",
        "    return md\n",
        "\n",
        "print(\"\n",
        "Markdown view for code run:\")\n",
        "display(Markdown(render_report_markdown(final_report_code)))\n",
        "\n",
        "print(\"\n",
        "Markdown view for deck run:\")\n",
        "display(Markdown(render_report_markdown(final_report_deck)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Simple Evaluation / Reflection",
        "The critique loop lets the evaluating agent request improvements until a report is satisfactory or the iteration limit is reached. Below we print how many iterations each demo consumed and show a tiny heuristic that checks whether higher-risk inputs produce higher scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Summarize critique iterations\n",
        "code_iterations = len(final_report_code.get(\"critique_history\", []))\n",
        "deck_iterations = len(final_report_deck.get(\"critique_history\", []))\n",
        "print(f\"Code report iterations: {code_iterations}\")\n",
        "print(f\"Deck report iterations: {deck_iterations}\")\n",
        "\n",
        "# Simple heuristic evaluation across synthetic cases\n",
        "synthetic_cases = [\n",
        "    {\n",
        "        \"description\": \"Low-risk code\",\n",
        "        \"metrics\": {\"pii_mentions\": [], \"log_statement_count\": 1, \"total_lines\": 10},\n",
        "        \"file_type\": \"code\",\n",
        "    },\n",
        "    {\n",
        "        \"description\": \"PII-heavy code\",\n",
        "        \"metrics\": {\"pii_mentions\": [\"email\", \"customer\"], \"log_statement_count\": 4, \"total_lines\": 50},\n",
        "        \"file_type\": \"code\",\n",
        "    },\n",
        "    {\n",
        "        \"description\": \"Risky deck\",\n",
        "        \"metrics\": {\"estimated_slides\": 5, \"contains_confidential\": True, \"risky_terms\": [\"guarantee\"]},\n",
        "        \"file_type\": \"pdf\",\n",
        "    },\n",
        "]\n",
        "\n",
        "for case in synthetic_cases:\n",
        "    payload = json.dumps(\n",
        "        {\n",
        "            \"company\": \"EvalCorp\",\n",
        "            \"file_path\": f\"/tmp/{case['description'].replace(' ', '_').lower()}\",\n",
        "            \"file_type\": case[\"file_type\"],\n",
        "            \"analysis_metrics\": case[\"metrics\"],\n",
        "            \"previous_feedback\": \"\",\n",
        "        }\n",
        "    )\n",
        "    report = json.loads(report_draft_tool_fn(payload))\n",
        "    print(f\"{case['description']}: risk_score={report['risk_score']}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}